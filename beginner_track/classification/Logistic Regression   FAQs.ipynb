{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAQs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is Logistic Regression?\n",
    "- Is a Binary classifier\n",
    "- Examines the linear relationship between features and label\n",
    "- Uses [Sigmoid Function](https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf) \n",
    "    to compute the probability for which categories an observation belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is Sigmoid Function?\n",
    "- Function convert any number to probabilty between 0 and 1\n",
    "![alt text](https://www.researchgate.net/profile/Knut_Kvaal/publication/239269767/figure/fig2/AS:643520205430784@1530438581076/An-illustration-of-the-signal-processing-in-a-sigmoid-function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is Label Encoding? Why we need to do?\n",
    "- Enumerate string labels to integer labels\n",
    "- Computers prefer to work with numbers\n",
    "- **Example:** Spam vs Non-Spam => 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is Explanatory Data Analysis (EDA)? And why?\n",
    "- Utilize tools to inspect and visualize data \n",
    "- In Python, use packages: Pandas, Seaborn or Matplotlib\n",
    "- Why? => Check odd values, missing values, etc. and look for hints to select algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Why split data into training and testing sets? Why splitting into 80 for training and 20 for testing?\n",
    "- Validate the performance on the Machine Learning algorithm for robustness\n",
    "- 80:20 Golden ratio to provide just enough data for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. How Logistic Regression training works?\n",
    "\n",
    "##### 1. Linear Formula\n",
    "**y = w*x + b** that \n",
    "* **x, y** are input and output\n",
    "* **w, b** are weight and bias that are initialized randomly. **GOAL** is to optimize **w, b** to minimize **LOSS**\n",
    "\n",
    "##### 2. Sigmoid Function\n",
    "- convert **y = f(x) = w*x + b** from **Linear Formula** above to probability between 0 and 1\n",
    "![alt text](https://www.researchgate.net/profile/Knut_Kvaal/publication/239269767/figure/fig2/AS:643520205430784@1530438581076/An-illustration-of-the-signal-processing-in-a-sigmoid-function.png)\n",
    "\n",
    "##### 3. Final formula of Losgistic Regression\n",
    "* **Logistic regresion: f(X)** = σ(X) = 1 / (1 + e^(-X))\n",
    "* **Sigmoid: σ(x)** = 1 / (1 + e^(-w*x - b))\n",
    "* **Linear formua: y** = w*x + b\n",
    "### => Logistic Regression y^ = 1 / (1 + e^(-w*x - b))\n",
    "\n",
    "##### 4. Cost (Loss) functions: Cross Entropy (aka Log Loss)\n",
    "![alt text](https://miro.medium.com/max/1122/1*KY8QcvVOz1sdUnVKKhxWyA@2x.png)\n",
    "\n",
    "##### 5. Update weights and optimize\n",
    "* With calculated loss, update weight **new_w = old_w +/- ɑ * loss [1]**\n",
    "\n",
    "* Apply Calculus, **compute derivative of [1] and find old_w that new_w is minized**.\n",
    "\n",
    "* Repeat this process over iterations, we got **Gradient Descent** that **cost** for each iteration will move to the **global minimum**\n",
    "\n",
    "![alt text](https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What is Overffitting and Underfitting? How to avoid them?\n",
    "\n",
    "---\n",
    "**Overfitting** If the model does really great on the training data but poorly on the testing data, this means you may have overfitted.\n",
    "  * **Reasons** : \n",
    "      * Mostly due to the limited dataset that the model cannot be trained to be generalized.\n",
    "      * In this notebook, due to the limited Iris dataset, the logistic model in this notebook may easily encouter **overfitting** when it makes predictions against unseen data.\n",
    "  * **Solutions** : \n",
    "      * Simplify ML algorithms/models\n",
    "      * Add regularization\n",
    "\n",
    "**Underfitting** is when your model does not capture all the detail of the data while overfitting is when your model captures too much detail and ends up showcasing random noise as well.\n",
    "  * **Reason** : happen due to \n",
    "      * The simplicity of Machine Learning algorithms\n",
    "      * Too few data features. \n",
    "  * **Solutions** : \n",
    "      * Build more complex ML algorithms/models\n",
    "      * Add model parameters and data features.\n",
    "\n",
    "  ![Overfitting & Underfitting](https://miro.medium.com/max/1125/1*_7OPgojau8hkiPUiHoGK_w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "* Logistic Regression in Sci-Kit Learn, https://blog.goodaudience.com/classifying-flowers-using-logistic-regression-in-sci-kit-learn-38262416e4c6\n",
    "* Sigmoid Function, https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
